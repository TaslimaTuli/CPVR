{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1350fbf",
   "metadata": {},
   "source": [
    "# Activation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c50ad953",
   "metadata": {},
   "source": [
    "Artificial neural networks are the backbone of deep learning, machine learning, and artificial intelligence. These networks consist of a series of interconnected nodes called neurons. Each neuron has a weight, bias, and an activation function. Activation functions play a crucial role in determining the output of a neuron and eventually the entire network. In this report, we will dive deep into the most commonly used activation functions - Step, Sigmoid, ReLU, SELU, ELU, and Tanh."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99265393",
   "metadata": {},
   "source": [
    "## Step Function:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0dbdfad",
   "metadata": {},
   "source": [
    "The step function is the simplest activation function. It takes an input value and returns either 0 or 1. If the input is greater than or equal to zero, it returns 1, otherwise 0. The step function is useful for binary classification problems where we want to classify an input as either a 0 or 1.\n",
    "\n",
    "However, the step function has some limitations. It is not differentiable, which makes it unsuitable for use in gradient-based optimization algorithms like backpropagation. Additionally, the output is not smooth, and it has a constant value for a range of inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dbb09e2",
   "metadata": {},
   "source": [
    "## Sigmoid Function:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c70bb0f",
   "metadata": {},
   "source": [
    "The sigmoid function is a popular activation function used in neural networks. It maps any input value to a value between 0 and 1. The output of the sigmoid function represents the probability of the input belonging to the positive class. The sigmoid function has a smooth derivative, which makes it suitable for use in gradient-based optimization algorithms like backpropagation.\n",
    "\n",
    "However, the sigmoid function has some limitations. When the input is too large or too small, the gradient of the sigmoid function becomes close to zero, which results in the vanishing gradient problem. This problem makes it challenging to train deep neural networks using the sigmoid function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a6ac8c",
   "metadata": {},
   "source": [
    "## ReLU Function:\n",
    "\n",
    "The Rectified Linear Unit (ReLU) function is a popular activation function used in deep learning. It maps any input value to a value between 0 and infinity. If the input is less than or equal to zero, the output is zero, and if the input is greater than zero, the output is equal to the input. The ReLU function has a smooth derivative, which makes it suitable for use in gradient-based optimization algorithms like backpropagation.\n",
    "\n",
    "\n",
    "The ReLU function has some advantages over other activation functions. It has a non-linear output, which helps in modeling complex relationships between inputs and outputs. Additionally, it is computationally efficient, which makes it suitable for use in large-scale neural networks.\n",
    "\n",
    "\n",
    "However, the ReLU function has some limitations. When the input is negative, the output is zero, which results in the dying ReLU problem. This problem makes it challenging to train deep neural networks using the ReLU function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e7c24f2",
   "metadata": {},
   "source": [
    "## SELU Function:\n",
    "\n",
    "The Scaled Exponential Linear Unit (SELU) function is a variation of the ReLU function. The SELU function has a smooth derivative. \n",
    "\n",
    "The SELU function has some advantages over other activation functions. It has a non-linear output, which helps in modeling complex relationships between inputs and outputs. Additionally, it is self-normalizing, which means that the output of the SELU function has zero mean and unit variance. This property helps in reducing the vanishing gradient and exploding gradient problems.\n",
    "\n",
    "\n",
    "However, the SELU function has some limitations. It requires the input data to be normalized, which can be challenging in some scenarios. Additionally, it is computationally expensive, which makes it unsuitable for use in large-scale neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e189ca7",
   "metadata": {},
   "source": [
    "## ELU Function:\n",
    "\n",
    "The Exponential Linear Unit (ELU) function is a variation of the ReLU function. It maps any input value to a value between negative infinity and infinity. If the input is less than or equal to zero, the output is a negative exponential function of the input, and if the input is greater than zero, the output is equal to the input.\n",
    "\n",
    "The ELU function has some advantages over other activation functions. It has a non-linear output, which helps in modeling complex relationships between inputs and outputs. Additionally, it has a smoother transition at zero than the ReLU function, which helps in reducing the dying ReLU problem.\n",
    "\n",
    "However, the ELU function has some limitations. When the input is negative, the output is a negative exponential function of the input, which can be computationally expensive."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c3b3d1e",
   "metadata": {},
   "source": [
    "## Tanh Function:\n",
    "\n",
    "The Hyperbolic Tangent (Tanh) function is a popular activation function used in neural networks. It maps any input value to a value between -1 and 1. The output of the tanh function represents the probability of the input belonging to the positive class."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cvpr tf2.4 py3.8",
   "language": "python",
   "name": "cvpr"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
